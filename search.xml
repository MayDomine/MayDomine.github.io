<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Spatial Transformer</title>
      <link href="/2021/07/22/Spatial-Transformer/"/>
      <url>/2021/07/22/Spatial-Transformer/</url>
      
        <content type="html"><![CDATA[<h2 id="先讲仿射变换和双线性插值怎么做"><a href="#先讲仿射变换和双线性插值怎么做" class="headerlink" title="先讲仿射变换和双线性插值怎么做"></a>先讲仿射变换和双线性插值怎么做</h2><p>就是把原本的坐标矩阵做一个线性变换，得到新的坐标矩阵，然后由于这个坐标矩阵里面有的不是整数，因此做双线性插值，对output的矩阵上每一个像素P(x,y)来说，它去变换出来的那小数矩阵上，根据公式计算它应该有的像素。</p><p><img src="https://raw.githubusercontent.com/MayDomine/Photo_for_blog/master/images/SouthEast.png" alt="Bi-linear Interpolation"></p><p><img src="https://raw.githubusercontent.com/MayDomine/Photo_for_blog/master/images/image-20210722165809211.png" alt="image-20210722165809211"></p><h2 id="然后再看STN"><a href="#然后再看STN" class="headerlink" title="然后再看STN"></a>然后再看STN</h2><p>由此可见三个部分：</p><ul><li><em><strong>Localisation</strong></em> <em><strong>net</strong></em> :<em><strong>参数预测</strong></em></li><li><em><strong>Gridgenerator</strong></em> :<em><strong>坐标映射</strong></em></li><li>***Sampler:***<em><strong>像素采集</strong></em></li></ul><p><img src="https://raw.githubusercontent.com/MayDomine/Photo_for_blog/master/images/image-20210722163222431.png" alt="image-20210722163222431"></p><p>听起来和玄乎，第一个参数预测其实就是前面说的仿射变换，对于图片来说，一个(2,3)的矩阵就可以完成各种变换(旋转，平移，放大，缩小)</p><p>第二个坐标映射就是 拿原本的坐标矩阵乘上仿射变换的矩阵，得到一个新的坐标矩阵 $X^S$</p><p>由于新的坐标矩阵可能有小数，要做处理，这里有两种方式，一种是向前映射，就是将新坐标对应的点(浮点数坐标的点)根据其周围四个点的位置，把自己的值按照权重分给周围四个点，最后结果是原本映射的像素点按照一定权重叠加出来的矩阵。</p><p>还有一个是向后映射，就是说我们最终output出来的这个新矩阵，它的shape我们是知道了，那根据它的每个点坐标，根据逆映射，可以找到对应原图像上的一个坐标(但是可能存在浮点数)，然后用这个浮点数坐标周围的四个点来进行插值，就能计算出一个个的像素。（我本来以为这个地方要对变换矩阵求逆，但是其实不用，在初始定义的时候，变换矩阵就没有卡死说是对谁的变换，而且参数也是随着梯度下降逐渐更新的，所以一开始做坐标映射的时候就向后映就ok了）</p><p>反向传播我没有推，但是据说这样的方法是可以正确回传的，放上公式把。</p><p><img src="https://gitee.com/shilongshen/image-bad/raw/master/img/20200711175159.png" alt="img"></p><p><a href="https://blog.csdn.net/qq_40901266/article/details/107301425?utm_medium=distribute.pc_relevant.none-task-blog-2~default~baidujs_baidulandingword~default-1.control&spm=1001.2101.3001.4242">参考</a></p><p>还有Pytorch官方的tutorial <a href="https://pytorch.org/tutorials/intermediate/spatial_transformer_tutorial.html">Spatial Transformer Networks Tutorial — PyTorch Tutorials 1.9.0+cu102 documentation</a></p><p>然后我在李宏毅2021的HW3里试了一下这个，但是由于HW3里本身就做过Data-aug，所以效果可能不是太好，而且具体超参数我也懒得调。</p><p>官方给的代码里面还是有很多东西的</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Net, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">10</span>, kernel_size=<span class="number">5</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">10</span>, <span class="number">20</span>, kernel_size=<span class="number">5</span>)</span><br><span class="line">        self.conv2_drop = nn.Dropout2d()</span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">320</span>, <span class="number">50</span>)</span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">50</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Spatial transformer localization-network 这里就是参数预测部分，可以看到其实接了两层卷积池化，这里的参数是决定后面变换矩阵参数的。</span></span><br><span class="line">        self.localization = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">1</span>, <span class="number">8</span>, kernel_size=<span class="number">7</span>), <span class="comment">#卷积层的第一个参数是in_channel的Dimension，第二个是out_channel的Dimension</span></span><br><span class="line">            <span class="comment">#Kernel_size是卷积核的大小</span></span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>, stride=<span class="number">2</span>),</span><br><span class="line">            nn.ReLU(<span class="literal">True</span>),</span><br><span class="line">            nn.Conv2d(<span class="number">8</span>, <span class="number">10</span>, kernel_size=<span class="number">5</span>),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>, stride=<span class="number">2</span>),</span><br><span class="line">            nn.ReLU(<span class="literal">True</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Regressor for the 3 * 2 affine matrix 这个矩阵对参数矩阵做变换，然后得到(N,2,3)的变换矩阵</span></span><br><span class="line">        self.fc_loc = nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="number">10</span> * <span class="number">3</span> * <span class="number">3</span>, <span class="number">32</span>),</span><br><span class="line">            nn.ReLU(<span class="literal">True</span>),</span><br><span class="line">            nn.Linear(<span class="number">32</span>, <span class="number">3</span> * <span class="number">2</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Initialize the weights/bias with identity transformation</span></span><br><span class="line">        self.fc_loc[<span class="number">2</span>].weight.data.zero_()</span><br><span class="line">        self.fc_loc[<span class="number">2</span>].bias.data.copy_(torch.tensor([<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>], dtype=torch.<span class="built_in">float</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Spatial transformer network forward function</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">stn</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="comment">#这里就是Spatial transformer的部分，先根据输入前传得到具体的theta矩阵</span></span><br><span class="line">        xs = self.localization(x)</span><br><span class="line">        xs = xs.view(-<span class="number">1</span>, <span class="number">10</span> * <span class="number">3</span> * <span class="number">3</span>)</span><br><span class="line">        theta = self.fc_loc(xs)</span><br><span class="line">        theta = theta.view(-<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"><span class="comment">#这里的两个函数相当于专为这个STN网络而生，第一个是根据输入形状和theta矩阵得到变化后矩阵的Index矩阵(就刚刚说的那个里面有小数的矩阵)</span></span><br><span class="line">        grid = F.affine_grid(theta, x.size())</span><br><span class="line">        <span class="comment">#这里把Index矩阵和输入一起输入，进行采样，然后出来的就是变化后的输入了。</span></span><br><span class="line">        x = F.grid_sample(x, grid)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="comment"># transform the input</span></span><br><span class="line">        x = self.stn(x)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Perform the usual forward pass</span></span><br><span class="line">        x = F.relu(F.max_pool2d(self.conv1(x), <span class="number">2</span>))</span><br><span class="line">        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), <span class="number">2</span>))</span><br><span class="line">        x = x.view(-<span class="number">1</span>, <span class="number">320</span>)</span><br><span class="line">        x = F.relu(self.fc1(x))</span><br><span class="line">        x = F.dropout(x, training=self.training)</span><br><span class="line">        x = self.fc2(x)</span><br><span class="line">        <span class="keyword">return</span> F.log_softmax(x, dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">model = Net().to(device)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CNN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Self-Attention for CNN</title>
      <link href="/2021/07/21/Transformer%E5%92%8CCNN%E7%9A%84%E5%85%B3%E7%B3%BB/"/>
      <url>/2021/07/21/Transformer%E5%92%8CCNN%E7%9A%84%E5%85%B3%E7%B3%BB/</url>
      
        <content type="html"><![CDATA[<p>$$Self-Attention(X)_t= softmax (A_t:)XWval$$</p><p>$$A := XWqryW_{key}^TXw_{val}$$</p><p>$X$​​为一个像素向量,$shape$​​大概长$(T , D_{in})$​这其中$T$​​为像素的数量，$D_{in}$应该为channel维度。</p><p>因此对于这样的输入序列，就算顺序被打乱，输出仍然是相同的</p><p>但在CV任务里，像素的分布是图片的信息之一，将一张图片的像素随机打乱后，就完全是另一张图片了。</p><p>因此要加入位置编码，输入关于位置的信息。</p><p>$$A : = ( X + P ) W _ { q r y } W _ { k e y } ^ { T } ( X + P ) ^ { T }$$</p><p>进一步，把self-attention扩展到多头注意力</p><blockquote><p>多头注意力：把原本的 $W_q,W_k,W_v$​​​​​拆成多个矩阵，再将输出拼接在一起，进行一次线性变换得到最终的output。</p></blockquote><p>$$M H S A ( X ) : =  \underset{h\in[N_h]}{concat} \quad [ Self-Attention( X )] \quad W _ { o u t } + b _ { o u t }$$​​​​</p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
            <tag> Transformer </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>第一篇：first of all</title>
      <link href="/2021/07/20/%E7%AC%AC%E4%B8%80%E7%AF%87%EF%BC%9Afirst%20of%20all/"/>
      <url>/2021/07/20/%E7%AC%AC%E4%B8%80%E7%AF%87%EF%BC%9Afirst%20of%20all/</url>
      
        <content type="html"><![CDATA[<h1 id="我可以用markdown么？"><a href="#我可以用markdown么？" class="headerlink" title="我可以用markdown么？"></a>我可以用markdown么？</h1><h2 id="我可以用markdown！"><a href="#我可以用markdown！" class="headerlink" title="我可以用markdown！"></a>我可以用markdown！</h2><h3 id="啊啊啊啊我有自己的博客了"><a href="#啊啊啊啊我有自己的博客了" class="headerlink" title="啊啊啊啊我有自己的博客了"></a>啊啊啊啊我有自己的博客了</h3><p>娘嘞！成为大牛指日可待！</p><p>博客是用git和hexo搭建的，然后用了hexo-orange的模板</p><p>后面打算做一些改动</p><ul><li><input disabled="" type="checkbox"> 啊这复选框好难打，notion一对中括号就好了</li><li><input disabled="" type="checkbox"> 首先换个别的水果的像素图，emmmm，也是水果，好像没什么创意</li><li><input disabled="" type="checkbox"> 然后改改颜色，虽然我觉得白色简约很好看</li><li><input disabled="" type="checkbox"> 然后搞两套post的模板</li><li><input disabled="" type="checkbox"> 现在的想法是，只记录关键的技术文档，其余的还是在notion上写，然后主页也会贴一个notion链接，还有就是更新一些大的人生动态，感觉树洞还是有洞比较好，notion更像是自我倾诉的垃圾桶，没人听的见。</li><li><input disabled="" type="checkbox"> 如果可以的话，以后接触前端的话，可以给这个主题的代码改改</li><li><input disabled="" type="checkbox"> 然后继续探索其他玩法，github yyds</li></ul><p>最后，push!</p>]]></content>
      
      
      <categories>
          
          <category> life </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 技术无关 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
